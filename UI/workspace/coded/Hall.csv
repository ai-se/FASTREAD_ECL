title,abstract,year,pdf,label,code,time
A Comparative Study of Ensemble Feature Selection Techniques for Software Defect Prediction,"Feature selection has become the essential step in many data mining applications. Using a single feature subset selection method may generate local optima. Ensembles of feature selection methods attempt to combine multiple feature selection methods instead of using a single one. We present a comprehensive empirical study examining 17 different ensembles of feature ranking techniques (rankers) including six commonly-used feature ranking techniques, the signal-to-noise filter technique, and 11 threshold-based feature ranking techniques. This study utilized 16 real-world software measurement data sets of different sizes and built 13,600 classification models. Experimental results indicate that ensembles of very few rankers are very effective and even better than ensembles of many or all rankers.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708824,yes,yes,5501
Attribute Selection and Imbalanced Data: Problems in Software Defect Prediction,"The data mining and machine learning community is often faced with two key problems: working with imbalanced data and selecting the best features for machine learning. This paper presents a process involving a feature selection technique for selecting the important attributes and a data sampling technique for addressing class imbalance. The application domain of this study is software engineering, more specifically, software quality prediction using classification models. When using feature selection and data sampling together, different scenarios should be considered. The four possible scenarios are: (1) feature selection based on original data, and modeling (defect prediction) based on original data; (2) feature selection based on original data, and modeling based on sampled data; (3) feature selection based on sampled data, and modeling based on original data; and (4) feature selection based on sampled data, and modeling based on sampled data. The research objective is to compare the software defect prediction performances of models based on the four scenarios. The case study consists of nine software measurement data sets obtained from the PROMISE software project repository. Empirical results suggest that feature selection based on sampled data performs significantly better than feature selection based on original data, and that defect prediction models perform similarly regardless of whether the training data was formed using sampled or original data.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670030,yes,yes,5513
Detecting Fault Modules Applying Feature Selection to Classifiers,"At present, automated data collection tools allow us to collect large amounts of information, not without associated problems. This paper, we apply feature selection to several software engineering databases selecting attributes with the final aim that project managers can have a better global vision of the data they manage. In this paper, we make use of attribute selection techniques in different datasets publicly available (PROMISE repository), and different data mining algorithms for classification to defect faulty modules. The results show that in general, smaller datasets with less attributes maintain or improve the prediction capability with less attributes than the original datasets.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296696,yes,yes,5612
The top ten list: dynamic fault prediction,"To remain competitive in the fast paced world of software development, managers must optimize the usage of their limited resources to deliver quality products on time and within budget. In this paper, we present an approach (the top ten list) which highlights to managers the ten most susceptible subsystems (directories) to have a fault. Managers can focus testing resources to the subsystems suggested by the list. The list is updated dynamically as the development of the system progresses. We present heuristics to create the top ten list and develop techniques to measure the performance of these heuristics. To validate our work, we apply our presented approach to six large open source projects (three operating systems: NetBSD, FreeBSD, OpenBSD; a window manager: KDE; an office productivity suite: KOffice; and a database management system: Postgres). Furthermore, we examine the benefits of increasing the size of the top ten list and study its performance.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510122,yes,yes,5626
A model for early prediction of faults in software systems,"Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict faulty modules in software data different techniques have been proposed which includes statistical method, machine learning methods, neural network techniques and clustering techniques. Predicting faults early in the software life cycle can be used to improve software process control and achieve high software reliability. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules using decision tree based Model in combination of K-means clustering as preprocessing technique. This approach has been tested with CM1 real time defect datasets of NASA software projects. The high accuracy of testing results show that the proposed Model can be used for the prediction of the fault proneness of software modules early in the software life cycle.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5451695,yes,yes,5628
"Defect Prediction using Combined Product and Project Metrics - A Case Study from the Open Source ""Apache"" MyFaces Project Family","The quality evaluation of open source software (OSS) products, e.g., defect estimation and prediction approaches of individual releases, gains importance with increasing OSS adoption in industry applications. Most empirical studies on the accuracy of defect prediction and software maintenance focus on product metrics as predictors that are available only when the product is finished. Only few prediction models consider information on the development process (project metrics) that seems relevant to quality improvement of the software product. In this paper, we investigate defect prediction with data from a family of widely used OSS projects based both on product and project metrics as well as on combinations of these metrics. Main results of data analysis are (a) a set of project metrics prior to product release that had strong correlation to potential defect growth between releases and (b) a combination of product and project metrics enables a more accurate defect prediction than the application of one single type of measurement. Thus, the combined application of project and product metrics can (a) improve the accuracy of defect prediction, (b) enable a better guidance of the release process from project management point of view, and (c) help identifying areas for product and process improvement.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725724,yes,yes,5630
Modeling the Effect of Size on Defect Proneness for Open-Source Software,"Quality is becoming increasingly important with the continuous adoption of open-source software. Previous research has found that there is generally a positive relationship between module size and defect proneness. Therefore, in open-source software development, it is important to monitor module size and understand its impact on defect proneness. However, traditional approaches to quality modeling, which measure specific system snapshots and obtain future defect counts, are not well suited because open-source modules usually evolve and their size changes over time. In this study, we used Cox proportional hazards modeling with recurrent events to study the effect of class size on defect-proneness in the Mozilla product. We found that the effect of size was significant, and we quantified this effect on defect proneness.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273266,yes,yes,5637
Project Data Incorporating Qualitative Factors for Improved Software Defect Prediction,"To make accurate predictions of attributes like defects found in complex software projects we need a rich set of process factors. We have developed a causal model that includes such process factors, both quantitative and qualitative. The factors in the model were identified as part of a major collaborative project. A challenge for such a model is getting the data needed to validate it. We present a dataset, elicited from 31 completed software projects in the consumer electronics industry, which we used for validation. The data were gathered using a questionnaire distributed to managers of recent projects. The dataset will be of interest to other researchers evaluating models with similar aims. We make both the dataset and causal model available for research use.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273258,yes,yes,5641
An application of zero-inflated Poisson regression for software fault prediction,"Poisson regression model is widely used in software quality modeling. When the response variable of a data set includes a large number of zeros, Poisson regression model will underestimate the probability of zeros. A zero-inflated model changes the mean structure of the pure Poisson model. The predictive quality is therefore improved. In this paper, we examine a full-scale industrial software system and develop two models, Poisson regression and zero-inflated Poisson regression. To our knowledge, this is the first study that introduces the zero-inflated Poisson regression model in software reliability. Comparing the predictive qualities of the two competing models, we conclude that for this system, the zero-inflated Poisson regression model is more appropriate in theory and practice.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989459,yes,yes,5649
A Density Based Clustering approach for early detection of fault prone modules,"Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict fault-proneness of modules different techniques have been proposed which includes statistical methods, machine learning techniques, neural network techniques and clustering techniques. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules by using Density Based Clustering technique. This approach has been tested with real time defect datasets of NASA software projects named as PC1. Predicting faults early in the software life cycle can be used to achieve high software quality. The results show that the fusion of requirement and code metric is the best prediction model for detecting the faults as compared with mostly used code based model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5559753,yes,yes,5650
Predicting faults using the complexity of code changes,"Predicting the incidence of faults in code has been commonly associated with measuring complexity. In this paper, we propose complexity metrics that are based on the code change process instead of on the code. We conjecture that a complex code change process negatively affects its product, i.e., the software system. We validate our hypothesis empirically through a case study using data derived from the change history for six large open source projects. Our case study shows that our change complexity metrics are better predictors of fault potential in comparison to other well-known historical predictors of faults, i.e., prior modifications and prior faults.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070510,yes,yes,5652
Assessing UML design metrics for predicting fault-prone classes in a Java system,"Identifying and fixing software problems before implementation are believed to be much cheaper than after implementation. Hence, it follows that predicting fault-proneness of software modules based on early software artifacts like software design is beneficial as it allows software engineers to perform early predictions to anticipate and avoid faults early enough. Taking this motivation into consideration, in this paper we evaluate the usefulness of UML design metrics to predict fault-proneness of Java classes. We use historical data of a significant industrial Java system to build and validate a UML-based prediction model. Based on the case study we have found that level of detail of messages and import coupling-both measured from sequence diagrams, are significant predictors of class fault-proneness. We also learn that the prediction model built exclusively using the UML design metrics demonstrates a better accuracy than the one built exclusively using code metrics.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463285,yes,yes,5658
Early Software Fault Prediction Using Real Time Defect Data,"Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict faulty modules in software data different techniques have been proposed which includes statistical method, machine learning methods, neural network techniques and clustering techniques. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules by using clustering techniques. This approach has been tested with three real time defect datasets of NASA software projects, JM1, PC1 and CM1. Predicting faults early in the software life cycle can be used to improve software process control and achieve high software reliability. The results show that when all the prediction techniques are evaluated, the best prediction model is found to be the fusion of requirement and code metric model.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381121,yes,yes,5700
Exploratory study of a UML metric for fault prediction,"This paper describes the use of a UML metric, an approximation of the CK-RFC metric, for predicting faulty classes before their implementation. We built a code-based prediction model of faulty classes using Logistic Regression. Then, we tested it in different projects, using on the one hand their UML metrics, and on the other hand their code metrics. To decrease the difference of values between UML and code measures, we normalized them using Linear Scaling to Unit Variance. Our results indicate that the proposed UML RFC metric can predict faulty code as well as its corresponding code metric does. Moreover, the normalization procedure used was of great utility, not just for enabling our UML metric to predict faulty code, using a code-based prediction model, but also for improving the prediction results across different packages and projects, using the same model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062214,yes,yes,5702
Fault Prediction using Early Lifecycle Data,"The prediction of fault-prone modules in a software project has been the topic of many studies. In this paper, we investigate whether metrics available early in the development lifecycle can be used to identify fault-prone software modules. More precisely, we build predictive models using the metrics that characterize textual requirements. We compare the performance of requirements-based models against the performance of code-based models and models that combine requirement and code metrics. Using a range of modeling techniques and the data from three NASA projects, our study indicates that the early lifecycle metrics can play an important role in project management, either by pointing to the need for increased quality monitoring during the development or by using the models to assign verification and validation activities.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402215,yes,yes,5704
Predicting Faults in High Assurance Software,"Reducing the number of latent software defects is a development goal that is particularly applicable to high assurance software systems. For such systems, the software measurement and defect data is highly skewed toward the not-fault-prone program modules, i.e., the number of fault-prone modules is relatively very small. The skewed data problem, also known as class imbalance, poses a unique challenge when training a software quality estimation model. However, practitioners and researchers often build defect prediction models without regard to the skewed data problem. In high assurance systems, the class imbalance problem must be addressed when building defect predictors. This study investigates the roughly balanced bagging (RBBag) algorithm for building software quality models with data sets that suffer from class imbalance. The algorithm combines bagging and data sampling into one technique. A case study of 15 software measurement data sets from different real-world high assurance systems is used in our investigation of the RBBag algorithm. Two commonly used classification algorithms in the software engineering domain, Naive Bayes and C4.5 decision tree, are combined with RBBag for building the software quality models. The results demonstrate that defect prediction models based on the RBBag algorithm significantly outperform models built without any bagging or data sampling. The RBBag algorithm provides the analyst with a tool for effectively addressing class imbalance when training defect predictors during high assurance software development.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634306,yes,yes,5706
Fault Discovery Protocol (FDP) for Passive Optical Networks (PONs),"Passive Optical Networks (PONs) are an attractive alternative to legacy copper based access lines, which still provide a number of services to customers despite their limited bandwidth and lack of future proof design. The recently growing volume of Ethernet PONs deployment [1], connected with low cost electronic and optical components used in the ONU modules, results in the situation where remote detection of faulty/active subscriber modules be-comes indispensable for proper operation of an EPON system. This paper addresses therefore the problem of the remote detection of faulty ONUs in the system, where the upstream channel is flooded with the Continuous Wave (CW) transmission from one or more damaged ONUs and standard communication is severed, providing a solution which is applicable in any type of PON networks, regardless of their operating protocol, physical structure, and data rate.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4381477,no,no,5445
Fault tolerant DTC for six-phase symmetrical induction machine,"In this paper, a new fault tolerant direct torque control (DTC) algorithm for six phase induction machines (6PIM) is introduced. The machine presents two sets of three phase windings spatially shifted by 60 electrical degrees. The aim of the proposed approach consists in computing an average stator voltage vector in order to control the mean values of the stator flux and the electromagnetic torque over a sampling period under open phase. The main advantages are fixed switching frequency, low torque ripples and reduced line current ripples in comparison with classical DTC. Simulation and experimental results show satisfying performances and validate the proposed method.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5415204,no,no,5447
Retrieval and small correction system for sailing directions using the Internet,"In this paper, we propose an automatic retrieval and small correction system for Japanese Sailing Directions using the Internet. There are two kinds of retrieval methods. One is the retrieval with a keyword, and the other is retrieval with ship's sailing route. The system is easily able to materialize automatic small corrections, for example revisions and additions for new navigational information. In the database of main body are divided many files. They have file names and ID. The ID is number of date and time. If there is different ID between own ship's database and the Maritime Safety Agency's one, the system execute automatically small corrections comparing with their names and ID under operation of the Internet. The programs of the system are composed Java applets, and they are easy to work in the Internet under httpd operation. These technologies are useful for LAN construction within a future ship. If the Maritime Safety Agencies in the world offer the HTML database and tables by using CD-ROM, our proposal system is made practicable as soon as possible",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953531,no,no,5448
State estimation of discrete-time Markov jump linear systems based on linear minimum mean-square error estimate,"This paper considers state estimation problem for discrete-time Markov jump linear systems. For this, two algorithm are presented. The first algorithm is an optimal algorithm of state estimation in the sense of linear minimum mean-square error estimate, which requires an ever-increasing computation and storage load with the length of the noise observation sequence. The second algorithm is a suboptimal algorithm which is proposed to reduce the computation and storage load of the optimal algorithm. A numerical example is presented to evaluate the performance of the proposed suboptimal algorithm.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5498592,no,no,5450
Hybrid Solution: A FEC Algorithm for Fault Tolerant Routing in Sensor Networks,"We study the characteristics of wireless sensor networks (WSN) and present a lightweight FEC coding algorithm combined with a smart fault tolerant routing scheme in this paper. The proposed coding-decoding algorithm is based on XOR operation and requires very little computation and storage space, which are critical for WSN. There are few existing channel coding algorithms (FEC) put forward for use in sensor networks, and they are not very suitable, due to their high computing, storage and delay cost. Further more, normal FEC coding algorithms are not flexible enough to suit the variable states in WSN. We adopt a cross-layer design wherein higher network layers use information about packet loss to adjust the coding level according to the dynamics of the network. And our routing scheme has the ability to discover and select robust paths to reliably relay data packets. Simulation result shows that our coding algorithm and self-adaptive routing scheme perform better than existing schemes.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4149829,no,no,5452
EVALUATION AND OPTIMISATION OF ERRORS IN THE REVERSE ENGINEERING PROCESS: A CASE STUDY,"Reverse engineering is used to reproduce a virtual model of any existing complex 3D shape. It is a fast evolving area which has a multitude of applications. It has also become an increasingly vital tool to reduce product development cycles. In conjunction with modern growing areas such as rapid prototyping or rapid tooling, this science is leading towards a rapidity, flexibility and agility discipline. This paper describes and analyses the successive errors embedded in the Reverse engineering process. Several simple components with specific geometric shapes are reverse engineered and remanufactured. Results show that the successive errors involved in each stage of the Reverse engineering process remain minimal (of the order of 0.5% or less) resulting in an overall maximum uncertainty of less than 1% between the original components and their remanufactured parts. Finally, two surface reconstruction procedures are described and compared with a suggested alternative method. This method enables the construction a CAD model with smooth surfaces without any oscillations and a closer fit to the scans. The final CAD model obtained can then be redesigned for an improved performance prior to a remanufacturing.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4286468,no,no,5453
A Radiation Hardened by Design Register File With Lightweight Error Detection and Correction,"A radiation hardened by design 32times36 b register file with error detection and correction (EDAC) capability is presented. The lightweight EDAC scheme (LEDAC) supports fine granularity (byte) writes, with low area and latency overhead, suitable for small, fast memories such as register file and first-level cache memory. The LEDAC scheme is described and its impact on memory efficiency and speed are quantified. The register file has been tested to be functional on a foundry 0.13 mum bulk CMOS process with a measured speed over 1 GHz at V<sub>DD</sub>=1.5 V. The LEDAC scheme is implemented in an external FPGA. Accelerated heavy ion testing results are also described. The experimentally measured RHBD register file SEE behavior is examined, and the proposed LEDAC scheme is shown to alleviate all soft errors in accelerated testing.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291761,no,no,5455
Teaching the Art of Fault Diagnosis in Electronics by a Virtual Learning Environment,"A virtual learning environment (VLE) to improve understanding of simple faul tfinding was created from a series of Web pages, an online quiz with automated marking, and a local-area-network-based simulator. It was tested on 57 first-year students (in 2002) and 69 students in 2003, taking a module in engineering design in electrical engineering in which a battery charger was designed and constructed. The results indicate that there was better than 100% improvement in the number of working battery chargers in both tested years. In addition, the students who used the VLE produced more working chargers and were better able to identify circuit blocks than those that did not. The learning approach is described by the adaptive character of thought cited in the present paper.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1495644,no,no,5457
Study of hybrid intelligent fault diagnosis,"A hybrid intelligent fault diagnosis method is presented for the diversity, uncertainty and complexity of device faults. This method integrates respective advantages of fault tree, fuzzy theory, neural networks and genetic algorithms to form a hybrid approach and is applied to fault diagnosis of fan. Experiments show that this method is simple and effective. It can also be applied to other fault diagnosis of complex systems and has certain portability.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456876,no,no,5458
Feature Selection with Imbalanced Data for Software Defect Prediction,"In this paper, we study the learning impact of data sampling followed by attribute selection on the classification models built with binary class imbalanced data within the scenario of software quality engineering. We use a wrapper-based attribute ranking technique to select a subset of attributes, and the random undersampling technique (RUS) on the majority class to alleviate the negative effects of imbalanced data on the prediction models. The datasets used in the empirical study were collected from numerous software projects. Five data preprocessing scenarios were explored in these experiments, including: (1) training on the original, unaltered fit dataset, (2) training on a sampled version of the fit dataset, (3) training on an unsampled version of the fit dataset using only the attributes chosen by feature selection based on the unsampled fit dataset, (4) training on an unsampled version of the fit dataset using only the attributes chosen by feature selection based on a sampled version of the fit dataset, and (5) training on a sampled version of the fit dataset using only the attributes chosen by feature selection based on the sampled version of the fit dataset. We compared the performances of the classification models constructed over these five different scenarios. The results demonstrate that the classification models constructed on the sampled fit data with or without feature selection (case 2 and case 5) significantly outperformed the classification models built with the other cases (unsampled fit data). Moreover, the two scenarios using sampled data (case 2 and case 5) showed very similar performances, but the subset of attributes (case 5) is only around 15% or 30% of the complete set of attributes (case 2).",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381844,no,no,5508
"ACE: an aggressive classifier ensemble with error detection, correction and cleansing","Learning from noisy data is a challenging and reality issue for real-world data mining applications. Common practices include data cleansing, error detection and classifier ensembling. The essential goal is to reduce noise impacts and enhance the learners built from the noise corrupted data, so as to benefit further data mining procedures. In this paper, we present a novel framework that unifies error detection, correction and data cleansing to build an aggressive classifier ensemble for effective learning from noisy data. Being aggressive, the classifier ensemble is built from the data that has been preprocessed by the data cleansing and correcting techniques. Experimental comparisons will demonstrate that such an aggressive classifier ensemble is superior to the model built from the original noisy data, and is more reliable in enhancing the learning theory extracted from noisy data sources, in comparison with simple data correction or cleansing efforts",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1562954,no,no,5511
Attributes Reduction Applied to Leather Defects Classification,"This paper presents a study on attributes reduction, comparing five discriminant analysis techniques: FisherFace, CLDA, DLDA, YLDA and KLDA. Attributes reduction has been applied to the problem of leather defect classification using four different classifiers: C4.5, kNN, Naive Bayes and Support Vector Machines. The results of several experiments on the performance of discriminant analysis applied to the problem of defect detection are reported.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720389,no,no,5515
Support Vector Machine for Mechanical Faults Diagnosis,"Aiming at the difficulty that Support Vector Machine (SVM) model selection of classification algorithm affect classification accuracy, it research relevant factors that influence the precision of fault classifiers based on the typical fault data samples obtained by experimental setup of rotor-bearing systems. The results show that different SVM classifiers, in which different kernel functions and different kernel functions parameters are adopted, will influence the precision of fault classifiers in conditions that fault data samples is small. It can be conveniently applied to choose appropriate kernel functions and kernel functions parameters in engineering application.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459108,no,no,5516
Multistrategy ensemble learning: reducing error by combining ensemble learning techniques,"Ensemble learning strategies, especially boosting and bagging decision trees, have demonstrated impressive capacities to improve the prediction accuracy of base learning algorithms. Further gains have been demonstrated by strategies that combine simple ensemble formation approaches. We investigate the hypothesis that the improvement in accuracy of multistrategy approaches to ensemble learning is due to an increase in the diversity of ensemble members that are formed. In addition, guided by this hypothesis, we develop three new multistrategy ensemble learning techniques. Experimental results in a wide variety of natural domains suggest that these multistrategy ensemble learning techniques are, on average, more accurate than their component ensemble learning techniques.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318582,no,no,5518
Kernel Classification via Integrated Squared Error,"Nonparametric kernel methods are widely used and proven to be successful in many statistical learning problems. Wellknown examples include the kernel density estimate (KDE) for density estimation and the support vector machine (SVM) for classification. We propose a kernel classifier that optimizes an integrated squared error (ISE) criterion based on a ""difference of densities"" formulation. Our classifier is sparse, like SVMs, and performs comparably to state-of-the-art kernel methods. Furthermore, and unlike SVMs, the ISE criterion does not require the user to set any unknown regularization parameters. As a consequence, classifier training is faster than for support vector methods.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4301366,no,no,5519
Defect Classification Algorithm for IC Photomask Based on PCA and SVM,"During IC photomask vision inspection, considering problem that fine image defectpsilas fineness, complex shape, extraction feature difficultly, and effect by noise easily, presented defect identification classification algorithm based on PCA (principal components analysis) and SVM (support vector machine). It resolved the problem that fine and complex defect was difficult to classify, by merits of the extracting image global feature with PCA, and high accuracy and generalization capability with SVM. Regard class distance as criterion to construct the binary tree in multi-class SVM classification algorithm. It resolved the problem that the structure of binary tree affected the accuracy of classifier, and upgraded defect classification accuracy finally. Experiments show that six defects classification accuracy by this method is up to 97.8%, higher than best accuracy 93.3% by BP network and 83.3% by method based on region. And the training and inspecting time is few. In result, itpsilas an effective method for fineness defect identification and classification.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4566204,no,no,5521
Effect of the feature vector size on the generalization error: the case of MLPNN and RBFNN classifiers,"In pattern recognition literature, it is well known that a finite number of training samples cause practical difficulties in designing a classifier. Moreover, the generalization error of the classifier tends to increase as the number of features gets large. We study the generalization error of several classifiers (MLPNN, RBFNN, K NN) in high dimensional spaces, under a practical condition: the ratio of the training sample to the dimensionality is small. Experimental results show that the generalization error of neuronal classifiers decreases as a function of dimensionality while it increases for statistical classifiers",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=906154,no,no,5522
Statistical classification of raw textile defects,"The problem of classification of defects occurring in a textile manufacture is addressed. A new classification scheme is devised in which different features, extracted from the gray level histogram, the shape, and cooccurrence matrices, are employed. These features are classified using a support vector machines (SVM) based framework, and an accurate analysis of different multiclass classification schemes and SVM parameters has been carried out. The system has been tested using two textile databases showing very promising results.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333765,no,no,5523
Attenuation correction in MR-PET scanners with segmented T1-weighted MR images,"Attenuation correction of PET data acquired in new hybrid MR-PET scanners which do not offer the possibility of a measured attenuation correction can be done in different ways. A previous report of our group described a method which used attenuation templates. The present study utilizes a new knowledge-based segmentation approach applied on T1-weighted MR images. It examines the position and the tissue membership of each voxel and segments the head volume into attenuation-differing regions: brain tissue, extracerebral soft tissue, skull, air-filled nasal and paranasal cavities as well as the mastoid process. To examine this new approach three groups of subjects having MRI and PET were chosen, the selection criterion being the different MR scanners, while the PET scanner was the ECAT HR+ in all cases: 1) four subjects with 1.5T MR images and CPFPX PET scans, 2) four subjects with 3T MR images and Altanserin PET scans, and 3) three brain tumor patients with 3T MR images from the hybrid MR-BrainPET scanner and FET PET scans. Furthermore, a single subject had 3T MR images, a FDG PET scan, and an additional CT scan. All segmented T1-weighted MR images were converted into attenuation maps for 511 KeV photons with coefficients of 0.096 1/cm for brain tissue, 0.146 1/cm for skull, 0.095 1/cm for soft tissue, 0.054 1/cm for the mastoid process, and 0.0 1/cm for nasal and paranasal cavities. The CT volume was also converted from the Hounsfield units into attenuation coefficients valid for 511 keV photons. The 12 segmented-based attenuation (SBA) maps as well as the CT-based attenuation (CBA) map were first filtered by a 3D Gaussian kernel of 10 mm filter width and then used to reconstruct the corresponding PET emission data. These were compared to the PET images attenuation corrected using the conventional PET-based transmission data (PBA). Relative differences (RD) were calculated from ROIs. For the single subject the RD of CBA data exhibit a mean of 1.66%?0.84% with a rang- from -0.88% to 3.42%, while the RD's mean of SBA data is 1.42%?2.61% (range from -4.12% to 4.66%). Comparing the results obtained with the SBA correction only, the RD for 1) range from -6.10% to 2.56% for cortical regions and from -6.99% to 5.64% for subcortical regions; for 2) they range from -7.33% to 2.33% for the cortical regions, subcortical ones being not drawn due to the not significant tracer uptake; for 3) the mean over the three subjects resulted in 0.89%?1.10% for ROIs at 48% threshold of the image's maximum and in 2.25%?1.50% for ROIs at 72% threshold. ROIs on the healthy contra-lateral grey matter show a mean of -3.24%?0.87%. In conclusion, the first attenuation correction results obtained with the new segmented-based method on a strongly heterogeneous collective are very promising. Further improvements of the method will be focused on the delineation of the skull.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5402034,no,no,5531
Emission-based scatter correction in SPECT imaging,"Scatter correction in single photon emission computed tomography (SPECT) has been focused on either using multiple-window acquisition technique or the scatter modeling technique in iterative image re-construction. We propose a technique that uses only the emission data for scatter correction in SPECT. We assume that the scatter data can be approximated by convolving the primary data with a scatter kernel followed by the normalization using the scatter-to-primary ratio (SPR). Since the emission data is the superposition of the primary data and the scatter data, the convolution normalization process approximately results in the sum of the scatter data and a convolved version of scatter data with the kernel. By applying a proper scaling factor, we can make the estimation approximately equal to or less than the scatter data anywhere in the projection domain. Phantom and patient cardiac SPECT studies show that using the proposed emission-based scatter estimation can effectively reduce the scatter-introduced background in the reconstructed images. And additionally, the computational time for scatter correction is negligible as compared to no scatter correction in iterative image reconstruction.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076070,no,no,5532
Scalable multiple description video coding for error-resilient transmission over hybrid networks,In this paper a scalable multiple description video coding approach based on embedded multiple description scalar quantization (EMDSQ) is presented. The proposed approach enables the progressive transmission of video over unreliable channels with variable bandwidth. Experimental results show that in lossy transmission conditions the proposed embedded multiple description coding system yields better rate-distortion performance compared to single description video coding and can efficiently sustain 20% of losses.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4559545,no,no,5533
Parameterization of a model-based 3-D PET scatter correction,"Parameterization of a fast implementation of the Ollinger (1996) model-based 3-D scatter correction method for positron emission tomography (PET) has been evaluated using measured phantom data acquired on a GE Advance PET imaging system. The Ollinger method explicitly estimates the 3-D single-scatter distribution using measured emission and transmission data and then estimates the multiple-scatter as a convolution of the single scatter. The main algorithm difference from that implemented by Ollinger is that the scatter correction does not explicitly compute scatter for azimuthal angles; rather, it determines 2-D scatter estimates for data within 2-D ""super-slices"" using as input data from the 3-D direct-plane (nonoblique) slices. These axial super-slice data are composed of data within a parameterized distance from the center of the super-slice. A model-based scatter correction method can be parameterized, and choice parameters may significantly change the behavior of the algorithm. Parameters studied in this work included transaxial image downsampling, the number of detectors to calculate scatter to, multiples kernel width and magnitude, the number and thickness of super-slices, and the number of scatter estimation iterations. Measured phantom data included imaging of the NEMA NU-2001 image quality (IQ) phantom, the IQ phantom with 2 cm extra water-equivalent tissue strapped around its circumference, and an attenuation phantom (20 cm uniform cylinder with Teflon, water and air inserts) with two 8 cm diameter water-filled nonradioactive arms placed by its side. For the IQ phantom data, a subset of NEMA NU-2001 measures were used to determine the contrast-to-noise ratio (CNR), lung residual bias, and background variability. For the attenuation phantom, region of interests (ROIs) were drawn on the nonradioactive compartments and on the background. These ROIs were analyzed for inter and intra-slice variation, background bias, and compartment-to-background ratio. In most cases, the algorithm was most sensitive to multiple-scatter parameterization and least sensitive to transaxial downsampling. The algorithm showed convergence by the second iteration for the metrics used in this study. Also, the range of the magnitude of change in the metrics analyzed was - small over all changes in parameterization. Further work to extend these results to more realistic phantom and clinical datasets is warranted.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1039554,no,no,5534
Correction for head movements in positron emission tomography using an optical motion-tracking system,"Methods capable of correcting for head motion in all six degrees of freedom have been proposed for positron emission tomography (PET) brain imaging but not yet demonstrated in human studies. These methods rely on the accurate measurement of head motion in relation to the reconstruction coordinate frame. We present methodology for the direct calibration of an optical motion-tracking system to the reconstruction coordinate frame using paired coordinate measurements obtained simultaneously from a PET scanner and tracking system. We also describe the implementation of motion correction, based on the multiple acquisition frame method originally described by Picard and Thompson (1997), using data provided by the motion tracking system. Effective compensation for multiple six-degree-of-freedom movements is demonstrated in dynamic PET scans of the Hoffman brain phantom and a normal volunteer. We conclude that reduced distortion and improved quantitative accuracy can be achieved with this method in PET brain studies degraded by head movements",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998691,no,no,5536
Alternative methods for attenuation correction for PET images in MR-PET scanners,"This paper describes and compares procedures to obtain attenuation maps used for the absorption correction (AC) of PET brain scans if a transmission scan is not available as in the case of future MR-PET scanners. A previously reported approach called MBA (MRT-based attenuation correction) used Tl- weighted MR images which were segmented into four tissue types representing brain tissue, bone, other tissue and sinus to which appropriate attenuation coefficients were assigned. In this work a template-based attenuation correction (TBA) is presented which applies an attenuation template to single subjects. A common attenuation template was created from transmission scans of 10 normal volunteers and spatially normalized to the SPM2 standard brain shape. For each subject the Tl-MR template of SPM2 was warped onto the subject's individual MR image. The resulting warping matrix was applied to the common attenuation template so that an attenuation map matching the subject's brain shape was obtained. The attenuation maps of MBA and TBA were forward projected into attenuation factors which were alternatively used for AC. FDG scans of four subjects were reconstructed after AC with MBA and TBA and compared to images whose ACs were based on conventional attenuation maps (PBA=PET-based attenuation correction). Using PBA as reference in a region of interest analysis, MBA and TBA showed similar under- and overestimation of the reconstructed radioactivity up to -10% and 9%, respectively. The procedure to obtain the attenuation template needs still some improvements. Nevertheless, the TBA method of attenuation correction is a promising alternative to MBA with its still complex and not yet resolved accurate segmentation of MR images.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437073,no,no,5537
Correction for head movements in positron emission tomography using an optical motion tracking system,"Methods capable of correcting for head motion in all six degrees of freedom have been proposed for PET brain imaging but not yet demonstrated in human studies. These methods rely on the accurate measurement of motion in a coordinate frame aligned with the scanner. We present methodology for the direct calibration of an optical motion tracking system to the reconstruction coordinate frame using paired coordinate measurements obtained simultaneously from a PET scanner and tracking system. We also describe the implementation of motion correction, based on the multiple acquisition frame method originally described by Y. Picard and C.J. Thompson (1997), using data provided by the motion tracking system. Effective compensation for multiple six degree-of-freedom movements is demonstrated in dynamic PET scans of the Hoffman brain phantom and a normal volunteer. We conclude that reduced distortion and improved quantitative accuracy can be achieved with this method in PET brain studies degraded by head movements",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949205,no,no,5538
Segmented attenuation correction using <sup>137</sup>Cs single photon transmission,"Using a <sup>137</sup>Cs single photon transmission source for transmission scanning allows a higher photon flux and thus, better transmission statistics compared to coincidence transmission scanning. However, <sup>137</sup>Cs suffers from a high scatter fraction as well as emission contamination, both leading to an underestimation of the attenuation values. On our NaI- and GSO-systems this is currently compensated by subtracting emission contamination, scatter-scaling and re-mapping. Histogram based segmentation, widely used to shorten the scan time on <sup>68</sup>Ge devices, inherently is capable to compensate for a potential bias in the attenuation values. We have investigated segmented attenuation correction for <sup>137</sup>Cs transmission scans with NaI(Tl) PET scanners in previous work, and came to the conclusion that our current processing was superior to the formerly used segmentation routine. In this paper we re-investigate segmentation, however, using a more sophisticated algorithm. Our focus was mainly to improve the accuracy of our transmission scans rather than shorten the scan times. However, the potential to reduce the scan duration was investigated as well",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1009237,no,no,5540
Template based attenuation correction for PET in MR-PET scanners,"This work investigates a template based procedure for attenuation correction (TBA) of PET scans acquired in future hybrid MR-PET scanners which will not offer a measured attenuation correction. A previous report of our group described a method (TBA-SPM) how individual attenuation maps can be obtained from an attenuation template which is spatially normalized to the SPM2 standard brain shape. Attenuation maps of females and males obtained from PET transmission scans were used as input for this template. The present study replaces the template referring to SPM2 by a female and a male attenuation template (fAT and mAT), each based on four measured attenuation images (TBA-f&m). The corresponding T1-MR templates (fMR and mMR) were also available. Thus, possible morphological gender-related differences, not considered when the standardized SPM2 <sup>1</sup>brain shape is used, may be taken into account. To examine this approach PET scans of 15 female and 15 male subjects of an ongoing study were attenuation corrected using the templates fAT and mAT. For this purpose and depending on the subjects gender the fMR or mMR templates were warped onto the individual MR image. The resulting warping matrix was applied to fAT or mAT, respectively. The individualized attenuation maps were used to reconstruct the PET emission data. These were compared to PET images attenuation corrected with the conventional PET based transmission data (PBA). While the relative differences between PBA and TBA=f&m reconstructed images averaged over each group and all regions of interest were 0.57%  3.76% for females and 0.59%  3.56% for males, the corresponding values obtained with the TBA-SPM method showed an overestimation with similar standard deviations (2.39%  3.76% for females and 2.42%  3.37% for males). In conclusion, the alternative gender-related template method TBA-f&m gives acceptable results with no significan- - t differences between the genders.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774281,no,no,5541
Comparing the effects of standard and segmented attenuation correction,"We have evaluated a segmentation algorithm developed by General Electric for the Advance positron emission scanner (PET). Phantom studies were performed to measure the accuracy in emission scans reconstructed with segmented, attenuation data as a function of transmission scan time. The results indicated errors of less than 2% will be made in emission scan data reconstructed with transmission scan times of 3 minutes. Based on the phantom results, 185 patient data sets were acquired with both long (15 min.) non-segmented and short (3 min.) segmented attenuation scans. Comparisons of scan data in foci of abnormal uptake yielded a correlation coefficient between long and short scan SUV maximum values of 0.99 and a mean absolute difference of 4.6%. The average SUV values in lung between long and short has a correlation coefficient of 0.99 and a mean absolute difference of 3.1%. The corresponding values from the liver had a correlation coefficient of 0.96 and mean absolute difference of 7.4%. Visual review by physicians noted minor differences, but when grading the images on a scale of 1 to 5, 91% of the time there was no difference. In all cases comparing the long and short attenuation and no abnormal sites were missed",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949993,no,no,5542
Mining Top-K Fault Tolerant Frequent Patterns with Sliding Windows in Data Streams,"Mining frequent patterns over streaming data has become an important research focus field with broad applications. However, the real-world data may be usually polluted by uncontrolled factors. Fault-tolerant frequent pattern can express more generalized information than frequent pattern which is absolutely matched. Therefore, a novel single-pass algorithm is proposed for efficiently mining top-k fault-tolerant frequent pattern from data streams without minimum support threshold specified by user. A novel data structure is developed for maintaining the essential information of itemsets generated so far. Experimental results show that the developed algorithm is an efficient method for mining top-k fault-tolerant frequent pattern from data streams.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5565961,no,no,5552
Short Paper: Data Mining-based Fault Prediction and Detection on the Grid,"This paper describes a novel approach to fault detection and prediction on the grid based on data mining techniques. Data mining techniques are here applied as a mean to effectively process the significant amount of captured data from grid sites, services, workflows and activities. The paper provides a first approach of proposed techniques in terms of its ability of utilizing relevant information and the fault tolerance requirements. Such approach is one intelligent, distributed framework of fault detection and prediction for anomaly and failed activity by using resource- and workflow-based information. We use fault predictions to improve the performance of the workflow execution by avoiding potential faults of activities",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652162,no,no,5554
Reconstruction corrections in advanced neutron tomography algorithms,"Volume reconstruction methods operate by splitting the three-dimensional (3-D) reconstruction space into two-dimensional (2-D) data slices. Any of several algorithms (like FBP) available can be used to reconstruct 2-D cross-sectional images, which can then be stacked to provide a 3-D representation of internal volume of the object. The reconstruction process takes advantage from the improvement of raw data quality: the data pre-processing (that involves digital filtering) ensures that the reconstruction from low-contrast images can be performed with good details, allowing a better signal-to-noise ratio. This work describes the software treatment on data coming from the tomography system installed at the TRIGA reactor of in the ENEA's Casaccia Research Centre, Rome, Italy.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1417175,no,no,5556
Data preprocessing for prediction of rerecirculating water chemistry faults,"The water quality data in some petrochemical company are stored in the lims database, in order to support the field operation's decision-making according to these data, it's necessary to do some appropriate data mining. However, the accuracy of the results of data mining directly associated with the quality of source data, so data preprocessing on the raw data is necessary in the data mining process. The process of data preprocessing is as follows: First, the classification of the raw data, here it is based on the frequency difference of the data acquisition. Second, data cleaning on the raw data, including cleaning the noisy data, missing data and redundant data (here mainly refers to the attribute redundancy).For the noisy data it take the method combining computer with artificial, for the missing data it mainly take the interpolation method using the mean data, and the redundancy property items were deleted; Third, data transformation for the later data processing. It mainly do the normalization to eliminate the difference of the dimension and magnitude on the raw data, so that all data can put together to make a comprehensive analysis. It uses the mean and standard deviation method to preprocessing the raw data and then make a compare of the result of the two different methods and it get the conclusion that the mean method is a better normalization method; fourth, to conduct data reduction, which refers to reduce the data storage space as far as possible while it must ensure the data integrity, it uses the principal component analysis method to do the job.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5622170,no,no,5559
"Diagnosis of stator, rotor and airgap eccentricity faults in three-phase induction motors based on the multiple reference frames theory","This paper describes the use of multiple reference frames for the diagnosis of stator, rotor, and eccentricity faults in line-fed and direct torque controlled (DTC) inverter-fed induction motors. The use of this new technique, which was proposed by the authors for the diagnosis of inter-turn short circuits, is extended for the detection and classification of different types of faults. Each fault causes a different disturbance or introduces different components in the motor supply currents. Based on the multiple reference frames theory, by choosing a proper reference frame, it is possible to transform each one of these current components to a d-q frame. In these d-q reference frames, those current harmonics will appear as constants, thus being easily measured, extracted or manipulated. Because each fault causes a different disturbance, the multiple reference frames technique can easily discriminate between different faults. Simulation and experimental results demonstrate the effectiveness of the proposed technique for the diagnosis of stator, rotor, and airgap eccentricity faults in three-phase induction motors. Moreover, due to the operating philosophy of the multiple reference frames technique, its integration into the control system of a DTC induction motor drive is a straightforward task and is briefly addressed at the end of the paper.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1257725,no,no,5604
Improving robustness of gene ranking by resampling and permutation based score correction and normalization,"Feature ranking, which ranks features via their individual importance, is one of the frequently used feature selection techniques. Traditional feature ranking criteria are apt to produce inconsistent ranking results even with light perturbations in training samples when applied to high dimensional and small-sized gene expression data. A widely used strategy for solving the inconsistencies is the multi-criterion combination. But one problem encountered in combining multiple criteria is the score normalization. In this paper, problems in existing methods are first analyzed, and a new gene importance transformation algorithm is then proposed. Experimental studies on three popular gene expression datasets show that the multi-criterion combination based on the proposed score correction and normalization produces gene rankings with improved robustness.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5706607,no,no,5607
Selecting Discrete and Continuous Features Based on Neighborhood Decision Error Minimization,"Feature selection plays an important role in pattern recognition and machine learning. Feature evaluation and classification complexity estimation arise as key issues in the construction of selection algorithms. To estimate classification complexity in different feature subspaces, a novel feature evaluation measure, called the neighborhood decision error rate (NDER), is proposed, which is applicable to both categorical and numerical features. We first introduce a neighborhood rough-set model to divide the sample set into decision positive regions and decision boundary regions. Then, the samples that fall within decision boundary regions are further grouped into recognizable and misclassified subsets based on class probabilities that occur in neighborhoods. The percentage of misclassified samples is viewed as the estimate of classification complexity of the corresponding feature subspaces. We present a forward greedy strategy for searching the feature subset, which minimizes the NDER and, correspondingly, minimizes the classification complexity of the selected feature subset. Both theoretical and experimental comparison with other feature selection algorithms shows that the proposed algorithm is effective for discrete and continuous features, as well as their mixture.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5166487,no,no,5609
Fault-Tolerant Distributed Stream Processing System,"Real-time data processing systems are more and more popular nowadays. Data warehouses not only collect terabytes of data, they also process endless data streams. To support such a situation, a data extraction process must become a continuous process also. Here a problem of a failure resistance arises. It is important not only to process a set of data on time, even more important is not to lose any data when a failure occurs. We achieve this by applying a redundant distributed stream processing. In this paper, we present a fault-tolerant system designed for processing data streams originating from geographically distributed sources",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1698373,no,no,5613
A Self-Controlled Power Factor Correction Single-Phase Boost Pre-Regulator,"This paper presents a strategy for controlling the input current of a single-phase boost PFC (power factor corrector). A sample of the input voltage is not necessary since it is naturally used as the reference current. Besides this, the model presents few simplifications, therefore, being more complete, taking better advantage of the natural characteristics of the converter and obtaining similar results, when compared to classic control, by simply using a proportional compensator. Some of the advantages of this strategy include greater robustness and simplicity, less susceptibility to noise and a smoother turn-on characteristic",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1581961,no,no,5615
Defect handling in medium and large open source projects,"Open source projects have resulted in numerous high-quality, widely used products. Understanding the defect-handling strategies such projects employ can help us use the publicly accessible defect data from these projects to provide valuable quality-improvement feedback and to better understand the defect characteristics for a wider variety of software products. We conducted a survey to understand defect handling in selected open source projects and compared the particular approaches taken in different projects. We focused on defect handling instead of the broader quality assurance activities other researchers have previously reported. Our results provided quantitative evidence about the current practice of defect handling in an important subset of open source projects.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1309647,no,no,5622
The challenge of accurate software project status reporting: a two-stage model incorporating status errors and reporting bias,"Software project managers perceive and report project status. Recognizing that their status perceptions might be wrong and that they may not faithfully report what they believe, leads to a natural question-how different is true software project status from reported status? Here, the authors construct a two-stage model which accounts for project manager errors in perception and bias that might be applied before reporting status to executives. They call the combined effect of errors in perception and bias, project status distortion . The probabilistic model has roots in information theory and uses discrete project status from traffic light reporting. The true statuses of projects of varying risk were elicited from a panel of five experts and formed the model input. The same experts estimated the frequency with which project managers make status errors, while the authors created different bias scenarios in order to investigate the impact of different bias levels. The true status estimates, error estimates, and bias levels allow calculation of perceived and reported status. The results indicate that at the early stage of the development process most software projects are already in trouble, that project managers are overly optimistic in their perceptions, and that executives receive status reports very different from reality, depending on the risk level of the project and the amount of bias applied by the project manager. Key findings suggest that executives should be skeptical of favorable status reports and that for higher risk projects executives should concentrate on decreasing bias if they are to improve the accuracy of project reporting.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1176874,no,no,5624
Modeling the Effect of Size on Defect Proneness for Open-Source Software,"Quality is becoming increasingly important with the continuous adoption of open-source software. Previous research has found that there is generally a positive relationship between module size and defect proneness. Therefore, in open-source software development, it is important to monitor module size and understand its impact on defect proneness. However, traditional approaches to quality modeling, which measure specific system snapshots and obtain future defect counts, are not well suited because open-source modules usually evolve and their size changes over time. In this study, we used Cox proportional hazards modeling with recurrent events to study the effect of class size on defect- proneness in the Mozilla product. We found that the effect of size was significant, and we quantified this effect on defect proneness.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222706,no,no,5633
"Using product, process, and execution metrics to predict fault-prone software modules with classification trees","Software-quality classification models can make predictions to guide improvement efforts to those modules that need it the most. Based on software metrics, a model can predict which modules will be considered fault-prone, or not. We consider a module fault-prone if any faults were discovered by customers. Useful predictions are contingent on the availability of candidate predictors that are actually related to faults discovered by customers. With a diverse set of candidate predictors in hand, classification-tree modeling is a robust technique for building such software quality models. This paper presents an empirical case study of four releases of a very large telecommunications system. The case study used the regression-tree algorithm in the S-Plus package and then applied our general decision rule to classify modules. Results showed that in addition to product metrics, process metrics and execution metrics were significant predictors of faults discovered by customers",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895475,no,no,5634
When process data quality affects the number of bugs: Correlations in software engineering datasets,"Software engineering process information extracted from version control systems and bug tracking databases are widely used in empirical software engineering. In prior work, we showed that these data are plagued by quality deficiencies, which vary in its characteristics across projects. In addition, we showed that those deficiencies in the form of bias do impact the results of studies in empirical software engineering. While these findings affect software engineering researchers the impact on practitioners has not yet been substantiated. In this paper we, therefore, explore (i) if the process data quality and characteristics have an influence on the bug fixing process and (ii) if the process quality as measured by the process data has an influence on the product (i.e., software) quality. Specifically, we analyze six Open Source as well as two Closed Source projects and show that process data quality and characteristics have an impact on the bug fixing process: the high rate of empty commit messages in Eclipse, for example, correlates with the bug report quality. We also show that the product quality - measured by number of bugs reported - is affected by process data quality measures. These findings have the potential to prompt practitioners to increase the quality of their software process and its associated data quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463286,no,no,5635
An empirical approach for software fault prediction,"Measuring software quality in terms of fault proneness of data can help the tomorrow's programmers to predict the fault prone areas in the projects before development. Knowing the faulty areas early from previous developed projects can be used to allocate experienced professionals for development of fault prone modules. Experienced persons can emphasize the faulty areas and can get the solutions in minimum time and budget that in turn increases software quality and customer satisfaction. We have used Fuzzy C Means clustering technique for the prediction of faulty/ non-faulty modules in the project. The datasets used for training and testing modules available from NASA projects namely CM1, PC1 and JM1 include requirement and code metrics which are then combined to get a combination metric model. These three models are then compared with each other and the results show that combination metric model is found to be the best prediction model among three. Also, this approach is compared with others in the literature and is proved to be more accurate. This approach has been implemented in MATLAB 7.9.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578698,no,no,5654
Metrics selection for fault-proneness prediction of software modules,"It would be valuable to use metrics to identify the fault-proneness of software modules. It is important to select the most appropriate particular metric subset for fault-proneness prediction. We proposed an approach of metrics selection, which firstly utilized the correlation analysis to eliminate the high the correlation metrics and then ranked the remaining metrics based on the gray relational analysis. Three classifiers, that were logistic regression model, NaiveBayes, and J48, were utilized to empirically investigate the usefulness of selected metrics. Our results, based on a public domain NASA data set, indicate that 1) proposed method for metrics selection is effective, and 2) using 3-4 metrics gets the balanced performance for fault-proneness prediction of software modules.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541206,no,no,5657
